{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4432e892",
   "metadata": {},
   "source": [
    "# OCR-mLLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd231b04",
   "metadata": {},
   "source": [
    "Before running this code you will need to set up your OpenAI & Gemini API keys. Here's how I did it:\n",
    "\n",
    "1. Create a new file in your root directory called `.env` (no prefix)\n",
    "2. Store your API keys with the following names: OPENAI_API_KEY, ANTHROPIC_API_KEY, and GOOGLE_API_KEY\n",
    "3. Create a virtual environment by typing the following commands into your terminal:\n",
    "    - ```python3 -m venv .venv```\n",
    "    - ```source .venv/bin/activate```\n",
    "    - ```pip install -r requirements.txt```\n",
    "4. After running the pipeline, type ```deactivate``` in your terminal to make everything go back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7cf4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002663b1",
   "metadata": {},
   "source": [
    "### a. Run this cell to ensure you have all the necessary directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3540d",
   "metadata": {},
   "source": [
    "Before running the cell make sure you have an images folder in your root directory to feed the images into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549ec682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "from venv import logger\n",
    "from file_retrieval import *\n",
    "from json_creation import *\n",
    "\n",
    "\n",
    "# Get the root directory of the project\n",
    "root_dir = Path.cwd().parent.parent\n",
    "\n",
    "doc_format = \"txt\"\n",
    "\n",
    "# Get the user's path for the images folder assuming all images are stored here in .png format\n",
    "source_dir = root_dir / \"data\" / \"pngs\"\n",
    "txt_source_dir = root_dir / \"results\" / \"txt\" / \"ocr-llm-img2txt\"\n",
    "\n",
    "# Get the user's path for the output folder, create one if it doesn't exist\n",
    "output_dir = root_dir / \"results\" / doc_format\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bm_output_dir = root_dir / \"benchmarking-results\"/ f\"{doc_format}-accuracy\"\n",
    "bm_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# llm_array = [\"gpt-4o\", \"gemini-2.5-flash\", \"claude-4-sonnet\"]\n",
    "llms = {\"openai\": \"gpt-4o\", \"google\": \"gemini-2.5-flash\"}\n",
    "\n",
    "def make_llm_dirs(llm_array, target_dir, doc_format):\n",
    "    for llm in llms.values():\n",
    "        if doc_format == \"txt\":\n",
    "            dir = target_dir / f\"ocr-img2txt\"\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"llm-img2{doc_format}\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"ocr-llm-img2{doc_format}\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            dir = target_dir / f\"llm-img2{doc_format}\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"llm-txt2{doc_format}\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "make_llm_dirs(llms, output_dir, doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859c660",
   "metadata": {},
   "source": [
    "### b. Setup API keys & image encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ae833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "import base64\n",
    "from txt_creation import *\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "gpt_client = OpenAI(api_key=openai_api_key)\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "claude_client = Anthropic(api_key=anthropic_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a5773",
   "metadata": {},
   "source": [
    "### c. Get image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed71bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Add all filenames in images directory into the `filenames` array with the ENTIRE filepath\n",
    "img_filepaths = []\n",
    "ocr_output_filepaths = []\n",
    "# count = 0\n",
    "for path in source_dir.iterdir():\n",
    "  if path.suffix.lower() == \".png\" and path.is_file():\n",
    "    img_filepaths.append(path)\n",
    "print(len(img_filepaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702eec5-e52d-4b51-8907-f593204a1b76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Run pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e34ea1-f6ae-4de7-9887-764da7178f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files from ocr-benchmarking/images folder & write to results folder\n",
    "for path in img_filepaths:\n",
    "    file_name = output_dir / \"ocr-img2txt\" / path.stem\n",
    "    file_name = str(file_name) + \".txt\"\n",
    "    \n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(pytesseract.image_to_string(Image.open(str(path)))) # TODO: Change config as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df39757",
   "metadata": {},
   "source": [
    "## 3. Prepare the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045337cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_ocr_llm = \"\"\"\n",
    "You are a text correction assistant. Your task is to clean up and correct errors from raw OCR output.\n",
    "The text may contain misrecognized characters, broken words, or incorrect formatting.\n",
    "Carefully read the provided OCR output and produce a corrected version that is grammatically accurate \n",
    "and as faithful to the original content as possible. Because this is a historical document, try to \n",
    "preserve archaic spelling or formatting where clearly intended. Only correct obvious OCR errors.\n",
    "Put the dates associated with each entry at the end of the line.\n",
    "\n",
    "Input (Raw OCR Text):\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_llm = \"\"\"\n",
    "You are an expert historian. Your task is to transcribe the provided image into text. The image\n",
    "is a 20th century bibliographic entry. Because this is a historical document, try to preserve \n",
    "archaic spelling or formatting where clearly intended. Put the dates associated with each entry at the end of the line.\n",
    "Return the text only, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# prompt_llm = \"\"\"\n",
    "# From the provided image, give me the first word and nothing else\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58cb0c-aeb8-47cc-9528-26bc3a802984",
   "metadata": {},
   "source": [
    "## 4. OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d64bb8",
   "metadata": {},
   "source": [
    "### (i) Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06535758",
   "metadata": {},
   "source": [
    "#### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163080e4-5134-407c-9cdd-7a89141e1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for path in img_filepaths:\n",
    "    if count == 2:\n",
    "        break\n",
    "    count += 1\n",
    "    input = \"\"\n",
    "    base64_image = encode_image(path)\n",
    "    ocr_text_path = str(output_dir / \"ocr-img2txt\" / path.stem) + \".txt\" # THIS REMAINS THE SAME b/c we're reading the OCR output\n",
    "    with open(ocr_text_path, 'r') as file:\n",
    "        input += file.read()\n",
    "    prompt_ocr_llm = prompt_template_ocr_llm.format(input=input).strip()\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_ocr_llm\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    with open(output_dir / f\"ocr-llm-img2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "        file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cafe77",
   "metadata": {},
   "source": [
    "#### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c2eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-30 15:56:55 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    base64_image = encode_image(path)\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_llm\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    with open(output_dir / f\"llm-img2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "        file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa37e9",
   "metadata": {},
   "source": [
    "#### c. OCR-LLM (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p062.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p060.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p048.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p049.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p061.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p059.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p058.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p039.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p038.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p035.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p034.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p036.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p037.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p033.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p042.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p056.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p057.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p043.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p055.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p041.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p040.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p054.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p050.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p044.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p045.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p051.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p047.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p053.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p052.txt'), PosixPath('/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p046.txt')]\n",
      "/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/txt/ocr-img2txt/kbaa-p062.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-10 11:14:35 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "ocr_output_filepaths = []\n",
    "for path in Path(output_dir/\"ocr-img2txt\").iterdir():\n",
    "   if path.suffix.lower() == \".txt\" and path.is_file():\n",
    "      ocr_output_filepaths.append(path)\n",
    "if doc_format == \"txt\":\n",
    "    await process_double_async(img_filepaths, ocr_output_filepaths, output_dir/\"ocr-llm-img2txt\", openai_img_txt2txt_async, doc_format, llms['openai'])\n",
    "else:\n",
    "    logger.info(\"Currently running %s for txt cell\", doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267fdde9",
   "metadata": {},
   "source": [
    "#### d. LLM (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0544747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-10 11:20:42 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletionMessage' object has no attribute 'conten'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc_format == \u001b[33m\"\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m process_single_async(img_filepaths, output_dir/\u001b[33m\"\u001b[39m\u001b[33mllm-img2txt\u001b[39m\u001b[33m\"\u001b[39m, openai_img2txt_async, doc_format, llms[\u001b[33m'\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      4\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCurrently running \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m for txt cell\u001b[39m\u001b[33m\"\u001b[39m, doc_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/txt_creation.py:121\u001b[39m, in \u001b[36mprocess_single_async\u001b[39m\u001b[34m(input_img_paths, output_dir, processor, doc_format, model)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Append the tasks to be executed outside the for loop\u001b[39;00m\n\u001b[32m    120\u001b[39m     tasks.append(processor(input_path, output_path))\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/txt_creation.py:63\u001b[39m, in \u001b[36mopenai_img2txt_async\u001b[39m\u001b[34m(input_img_path, output_path)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Async file write\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiofiles.open(output_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m f.write(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconten\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ChatCompletionMessage' object has no attribute 'conten'"
     ]
    }
   ],
   "source": [
    "if doc_format == \"txt\":\n",
    "    await process_single_async(img_filepaths, output_dir/\"llm-img2txt\", openai_img2txt_async, doc_format, llms['openai'])\n",
    "else:\n",
    "    logger.info(\"Currently running %s for txt cell\", doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5692d2",
   "metadata": {},
   "source": [
    "### (ii) JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd528634",
   "metadata": {},
   "source": [
    "#### a. Image to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/data/pngs/kbaa-p038.png\n",
      "Output path: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/json/llm-img2json/gpt-4o/kbaa-p038.json\n",
      "Image path /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/data/pngs/kbaa-p039.png\n",
      "Output path: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/json/llm-img2json/gpt-4o/kbaa-p039.json\n",
      "Image path /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/data/pngs/kbaa-p062.png\n",
      "Output path: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/json/llm-img2json/gpt-4o/kbaa-p062.json\n",
      "Image path /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/data/pngs/kbaa-p049.png\n"
     ]
    }
   ],
   "source": [
    "if doc_format == \"json\":\n",
    "    count = 0\n",
    "    for path in img_filepaths:\n",
    "        print(\"Image path\", path)\n",
    "        if count == 2:\n",
    "            break\n",
    "        count += 1\n",
    "        response = openai_img2json(path)\n",
    "        with open(output_dir / f\"llm-img2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "            print(\"Output path:\", output_dir / f\"llm-img2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"))\n",
    "            file.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ccf05",
   "metadata": {},
   "source": [
    "#### b. Text to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edaf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to c:\\Users\\vriez\\OneDrive\\Desktop\\Summer MAP\\ocr-benchmarking-1\\results\\json\\llm-txt2json\\gpt-4o\\kbaa-p003.json\n"
     ]
    }
   ],
   "source": [
    "if doc_format == \"json\":\n",
    "    count = 0\n",
    "    for path in img_filepaths:\n",
    "        \n",
    "        ocr_text_path = str(root_dir / \"results\" / \"txt\" / \"ocr-llm-img2txt\" / \"gpt-4o\" /path.stem) + \".txt\" # THIS REMAINS THE SAME b/c we're reading the OCR output\n",
    "        if count == 1:\n",
    "            break\n",
    "        count += 1\n",
    "        #response = openai_txt2json(ocr_text_path.replace(\"json\", \"txt\"))\n",
    "        response = openai_txt2json(ocr_text_path)\n",
    "        with open(output_dir / f\"llm-txt2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "            print(\"Writing to\", output_dir / f\"llm-txt2{doc_format}\" / \"gpt-4o\" / Path(path.stem + f\".{doc_format}\"))\n",
    "            file.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ed85e",
   "metadata": {},
   "source": [
    "#### c. Image to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfa470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][file retrieval] 2025-07-08 14:09:12 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 1/1 [01:13<00:00, 73.49s/it]\n"
     ]
    }
   ],
   "source": [
    "if doc_format == \"json\":\n",
    "    await process_json_async(img_filepaths, output_dir/\"llm_img2json\", openai_img2json_async, doc_format, llms['openai'])\n",
    "else:\n",
    "    logger.info(\"Currently running %s for JSON cell\", doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ae2c9",
   "metadata": {},
   "source": [
    "#### d. Text to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-08 13:54:33 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-07-08 13:55:03 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:55:03 [INFO] AFC remote call 1 is done.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object Entries can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     txt_filepaths = get_paths(\u001b[38;5;28mdir\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Call the main function that concurrently runs relevant async function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m process_txt2json_async(txt_filepaths, output_dir, gemini_txt2json_async, doc_format, llms[\u001b[33m'\u001b[39m\u001b[33mgoogle\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     10\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCurrently running \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m for JSON cell\u001b[39m\u001b[33m\"\u001b[39m, doc_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/json_creation.py:374\u001b[39m, in \u001b[36mprocess_txt2json_async\u001b[39m\u001b[34m(input_txt_paths, output_dir, processor, doc_format, model)\u001b[39m\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Append the tasks to be executed outside the for loop\u001b[39;00m\n\u001b[32m    373\u001b[39m     tasks.append(processor(input_path, output_path))\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/json_creation.py:228\u001b[39m, in \u001b[36mgemini_txt2json_async\u001b[39m\u001b[34m(input_path, output_path)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiofiles.open(input_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    226\u001b[39m     text = \u001b[38;5;28;01mawait\u001b[39;00m f.read()\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m entries = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    229\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    230\u001b[39m     response_model=Entries,\n\u001b[32m    231\u001b[39m     messages=[\n\u001b[32m    232\u001b[39m         {\n\u001b[32m    233\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mConvert each entry in this bibliography into structured JSON:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    235\u001b[39m             + text,\n\u001b[32m    236\u001b[39m         }\n\u001b[32m    237\u001b[39m     ],\n\u001b[32m    238\u001b[39m )\n\u001b[32m    240\u001b[39m json_result = entries.model_dump_json(indent=\u001b[32m2\u001b[39m, exclude_defaults=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Async file write\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: object Entries can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "if doc_format == \"json\":\n",
    "    dir = txt_source_dir / llms['openai'] # where to look for ocr-llm-img2txt output\n",
    "\n",
    "    # Get the text paths from ocr-llm-img2txt/gpt-4o directory\n",
    "    txt_filepaths = get_paths(dir, \"txt\")\n",
    "\n",
    "    # Call the main function that concurrently runs relevant async function\n",
    "    await process_json_async(txt_filepaths, output_dir/\"llm-txt2json\", openai_txt2json_async, doc_format, llms['openai'])\n",
    "else:\n",
    "    logger.info(\"Currently running %s for JSON cell\", doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ecae5",
   "metadata": {},
   "source": [
    "## 5. Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fb167",
   "metadata": {},
   "source": [
    "### (i) Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10c212",
   "metadata": {},
   "source": [
    "#### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e5d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-08 13:51:01 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:03 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH88hOaSPV-VkL8Enj6XIpgpDY27hOaekCH3fcu6dI9tHGMs6-zLqiteJlH6hYS4TSBpKqIpQuRNRBo-7ZyPLLOlRIwFmbL7RstftYHhvHdQ&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:03 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-07-08 13:51:20 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:20 [INFO] AFC remote call 1 is done.\n",
      "[file retrieval] 2025-07-08 13:51:20 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:23 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH88uHfAIs1qud0mBg1U7oyh1Gl7uU57yRvRxlPaOfleOt_OAOsVLRxDgIxYm2BhDWMhu3UdqMv1ruvyPdgbyO-SfD5DjBIEfmzrmaZkc1Z8&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:23 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-07-08 13:51:52 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 13:51:52 [INFO] AFC remote call 1 is done.\n",
      "[file retrieval] 2025-07-08 13:51:52 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m img_filepaths:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     my_file = \u001b[43mgemini_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     ocr_text_path = \u001b[38;5;28mstr\u001b[39m(output_dir / \u001b[33m\"\u001b[39m\u001b[33mocr-img2txt\u001b[39m\u001b[33m\"\u001b[39m / path.stem) + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# THIS REMAINS THE SAME b/c we're reading the OCR output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/google/genai/files.py:675\u001b[39m, in \u001b[36mFiles.upload\u001b[39m\u001b[34m(self, file, config)\u001b[39m\n\u001b[32m    671\u001b[39m   return_file = \u001b[38;5;28mself\u001b[39m._api_client.upload_file(\n\u001b[32m    672\u001b[39m       file, upload_url, file_obj.size_bytes, http_options=http_options\n\u001b[32m    673\u001b[39m   )\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m   return_file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m types.File._from_response(\n\u001b[32m    680\u001b[39m     response=_File_from_mldev(return_file.json[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    681\u001b[39m     kwargs=config_model.model_dump() \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[32m    682\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1073\u001b[39m, in \u001b[36mBaseApiClient.upload_file\u001b[39m\u001b[34m(self, file_path, upload_url, upload_size, http_options)\u001b[39m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1072\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_fd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1130\u001b[39m, in \u001b[36mBaseApiClient._upload_fd\u001b[39m\u001b[34m(self, file, upload_url, upload_size, http_options)\u001b[39m\n\u001b[32m   1128\u001b[39m retry_count = \u001b[32m0\u001b[39m\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count < MAX_RETRY_COUNT:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_in_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m response.headers.get(\u001b[33m'\u001b[39m\u001b[33mx-goog-upload-status\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1138\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    my_file = gemini_client.files.upload(file=path)\n",
    "    input = \"\"\n",
    "    ocr_text_path = str(output_dir / \"ocr-img2txt\" / path.stem) + \".txt\" # THIS REMAINS THE SAME b/c we're reading the OCR output\n",
    "    with open(ocr_text_path, 'r') as file:\n",
    "        input += file.read()\n",
    "    prompt_ocr_llm = prompt_template_ocr_llm.format(input=input).strip()\n",
    "\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=[\n",
    "            prompt_ocr_llm,\n",
    "            my_file\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(output_dir / f\"ocr-llm-img2{doc_format}\" / \"gemini-2.5-flash\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d3100",
   "metadata": {},
   "source": [
    "#### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-30 15:57:11 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-30 15:57:11 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8-acDCIOvz27fq2eb2vZioJCF8BN-044zRuKKM1P7IZM9f8JT8PaT8jpwpouYDaJ1Mn8kXyGt_zkX_1P0JMEH1MWeAdikdPHQ1A_MEGnfk&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-30 15:57:14 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8-acDCIOvz27fq2eb2vZioJCF8BN-044zRuKKM1P7IZM9f8JT8PaT8jpwpouYDaJ1Mn8kXyGt_zkX_1P0JMEH1MWeAdikdPHQ1A_MEGnfk&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-30 15:57:14 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-06-30 15:57:18 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-30 15:57:18 [INFO] AFC remote call 1 is done.\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    my_file = gemini_client.files.upload(file=path)\n",
    "\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=[\n",
    "            prompt_llm,\n",
    "            my_file\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(output_dir / f\"llm-img2{doc_format}\" / \"gemini-2.5-flash\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cec32",
   "metadata": {},
   "source": [
    "### (ii) JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53393d",
   "metadata": {},
   "source": [
    "#### a. Image to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17fe4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doc_format == \"json\":\n",
    "    count = 0\n",
    "    for path in img_filepaths:\n",
    "        if count == 1:\n",
    "            break\n",
    "        count += 1\n",
    "        response = gemini_img2json(path)\n",
    "        with open(output_dir / f\"llm-img2{doc_format}\" / \"gemini-2.5-flash\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "            file.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b78765",
   "metadata": {},
   "source": [
    "#### b. Text to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5055dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doc_format == \"json\":\n",
    "    count = 0\n",
    "    for path in img_filepaths:\n",
    "        ocr_text_path = str(root_dir/ \"results\" / \"txt\" / \"ocr-llm-img2txt\" / \"gemini-2.5-flash\" / path.stem) + \".txt\" # THIS REMAINS THE SAME b/c we're reading the OCR output\n",
    "        if count == 1:\n",
    "            break\n",
    "        count += 1\n",
    "        response = gemini_txt2json(ocr_text_path.replace(\"json\", \"txt\"))\n",
    "        with open(output_dir / f\"llm-txt2{doc_format}\" / \"gemini-2.5-flash\" / Path(path.stem + f\".{doc_format}\"), 'w') as file:\n",
    "            file.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d6b4d",
   "metadata": {},
   "source": [
    "#### d. Text to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42c3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-08 16:12:16 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-07-08 16:12:54 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-07-08 16:12:54 [INFO] AFC remote call 1 is done.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object Entries can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     txt_filepaths = get_paths(\u001b[38;5;28mdir\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Call the main function that concurrently runs relevant async function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m process_txt2json_async(txt_filepaths, output_dir, gemini_txt2json_async, doc_format, llms[\u001b[33m'\u001b[39m\u001b[33mgoogle\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     10\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCurrently running \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m for JSON cell\u001b[39m\u001b[33m\"\u001b[39m, doc_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/json_creation.py:374\u001b[39m, in \u001b[36mprocess_txt2json_async\u001b[39m\u001b[34m(input_txt_paths, output_dir, processor, doc_format, model)\u001b[39m\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Append the tasks to be executed outside the for loop\u001b[39;00m\n\u001b[32m    373\u001b[39m     tasks.append(processor(input_path, output_path))\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/map2025/ocr-benchmarking/src/workflow/json_creation.py:228\u001b[39m, in \u001b[36mgemini_txt2json_async\u001b[39m\u001b[34m(input_path, output_path)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiofiles.open(input_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    226\u001b[39m     text = \u001b[38;5;28;01mawait\u001b[39;00m f.read()\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m entries = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    229\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    230\u001b[39m     response_model=Entries,\n\u001b[32m    231\u001b[39m     messages=[\n\u001b[32m    232\u001b[39m         {\n\u001b[32m    233\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mConvert each entry in this bibliography into structured JSON:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    235\u001b[39m             + text,\n\u001b[32m    236\u001b[39m         }\n\u001b[32m    237\u001b[39m     ],\n\u001b[32m    238\u001b[39m )\n\u001b[32m    240\u001b[39m json_result = entries.model_dump_json(indent=\u001b[32m2\u001b[39m, exclude_defaults=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Async file write\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: object Entries can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "if doc_format == \"json\":\n",
    "    dir = txt_source_dir / llms['google'] # where to look for ocr-llm-img2txt output\n",
    "\n",
    "    # Get the text paths from ocr-llm-img2txt/gpt-4o directory\n",
    "    txt_filepaths = get_paths(dir, \"txt\")\n",
    "\n",
    "    # Call the main function that concurrently runs relevant async function\n",
    "    await process_txt2json_async(txt_filepaths, output_dir, gemini_txt2json_async, doc_format, llms['google'])\n",
    "else:\n",
    "    logger.info(\"Currently running %s for JSON cell\", doc_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d06c34",
   "metadata": {},
   "source": [
    "## 6. Send to Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0869c81",
   "metadata": {},
   "source": [
    "### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in img_filepaths:\n",
    "#     base64_image = encode_image(path)\n",
    "\n",
    "#     response = claude_client.messages.create(\n",
    "#         model='claude-opus-4-20250514',\n",
    "#         temperature=0,\n",
    "#         max_tokens=10,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\", \n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt_ocr_llm\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": \"image/png\",\n",
    "#                             \"data\": base64_image\n",
    "#                         }\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#             ]\n",
    "#     )\n",
    "#     print(response)\n",
    "\n",
    "#     with open(txt_output_dir / \"ocr-llm-img2txt\" / \"claude-4-sonnet\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "#         file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610ccd9",
   "metadata": {},
   "source": [
    "### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in img_filepaths:\n",
    "#     base64_image = encode_image(path)\n",
    "\n",
    "#     response = claude_client.messages.create(\n",
    "#         model='claude-opus-4-20250514',\n",
    "#         temperature=0,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\", \n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt_llm\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": \"image/png\",\n",
    "#                             \"data\": base64_image\n",
    "#                         }\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#             ]\n",
    "#     )\n",
    "\n",
    "#     with open(txt_output_dir / \"llm-img2txt\" / \"claude-4-sonnet\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "#         file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e20262",
   "metadata": {},
   "source": [
    "## 7. Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c21953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Script directory: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/src/workflow\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Project root: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Found ground-truth txt files: ['/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/data/ground-truth/json/gt_kbaa-p038.json']\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Found file names: ['gt_kbaa-p038']\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Models found: [('llm-img2json', 'gemini-2.5-flash'), ('llm-txt2json', 'gemini-2.5-flash'), ('llm-img2json', 'gpt-4o'), ('llm-txt2json', 'gpt-4o')]\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collecting results for model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collected results for model_type: llm-img2json, model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collecting results for model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collected results for model_type: llm-txt2json, model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collecting results for model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collected results for model_type: llm-img2json, model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collecting results for model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Collected results for model_type: llm-txt2json, model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for model_type: llm-img2json, model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for document: kbaa-p038\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for model_type: llm-txt2json, model: gemini-2.5-flash\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for document: kbaa-p038\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for model_type: llm-img2json, model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for document: kbaa-p038\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for model_type: llm-txt2json, model: gpt-4o\n",
      "[file retrieval] 2025-07-08 10:25:57 [INFO] Computing metrics for document: kbaa-p038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/json/llm-img2json\n",
      "/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/json/llm-txt2json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().\n",
    "parent))\n",
    "from benchmarking.txt_accuracy import clean_text_normalized, clean_text_nonorm, compute_metrics, build_dataframe\n",
    "from tools.file_retrieval import get_doc_names, get_docs, get_all_models\n",
    "from datetime import datetime\n",
    "from venv import logger\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Prerequisites:\n",
    "    - Ground truth text files located at `project_root/ground-truth/txt/kbaa-p#xyz.txt`\n",
    "    - LLM/OCR transcribed files located at:\n",
    "        - for LLM transcriptions: `project_root/results/llm_img2txt/<MODEL-NAME>/kbaa-p#xyz.txt`\n",
    "        - for OCR transcriptions: `project_root/results/ocr_img2txt/<MODEL-NAME>/kbaa-p#xyz.txt`\n",
    "\n",
    "    The main function will:\n",
    "    - Gather all ground truth text files\n",
    "    - For each ground truth text file and for each LLM/OCR model, gather the corresponding transcription\n",
    "    - Clean all the text files (normalized and not normalized)\n",
    "    - Compute metrics for each file and model\n",
    "    - Save results in two CSV files (one for normalized, one for non-normalized)\n",
    "        - Results are saved in `project_root/benchmarking-results/txt-accuracy`\n",
    "    \"\"\"\n",
    "\n",
    "    # =============\n",
    "    # Preliminaries\n",
    "    # =============\n",
    "\n",
    "    # args = parse_arguments()\n",
    "\n",
    "    script_dir = str(Path.cwd())\n",
    "    project_root = str(root_dir)\n",
    "    logger.info(\"Script directory: %s\", script_dir)\n",
    "    logger.info(\"Project root: %s\", project_root)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth_dir = root_dir / \"data\" / \"ground-truth\" / doc_format\n",
    "    doc_names = get_doc_names(ground_truth_dir, doc_format, keep_prefix=False)\n",
    "    doc_names = ['kbaa-p038']\n",
    "\n",
    "    # results/ paths\n",
    "    if doc_format == \"txt\":\n",
    "        all_models = get_all_models(\n",
    "            doc_format,\n",
    "            os.path.join(output_dir, f\"llm-img2{doc_format}\"),\n",
    "            os.path.join(output_dir, \"ocr-img2txt\"),\n",
    "            os.path.join(output_dir, f\"ocr-llm-img2{doc_format}\"),\n",
    "        )\n",
    "    else:\n",
    "        all_models = get_all_models(\n",
    "            doc_format,\n",
    "            os.path.join(output_dir, f\"llm-img2{doc_format}\"),\n",
    "            os.path.join(output_dir, f\"llm-txt2{doc_format}\"),\n",
    "        )\n",
    "    logger.info(f\"Models found: {all_models}\")\n",
    "\n",
    "    # ===========\n",
    "    # Gather files\n",
    "    # ===========\n",
    "\n",
    "    # -> Gather ground truths and put into dict:\n",
    "    ground_truths, all_texts = get_docs(ground_truth_dir, doc_names, doc_format, name_has_prefix=True)\n",
    "    ground_truths[\"__ALL__\"] = all_texts\n",
    "    if doc_format == \"txt\":\n",
    "        doc_lengths_normalized = {\n",
    "            doc: len(clean_text_normalized(text)) for doc, text in ground_truths.items()\n",
    "        }\n",
    "        doc_lengths_nonorm = {\n",
    "            doc: len(clean_text_nonorm(text)) for doc, text in ground_truths.items()\n",
    "        }\n",
    "        total_doc_len_normalized = len(clean_text_normalized(ground_truths[\"__ALL__\"]))\n",
    "        total_doc_len_nonorm = len(clean_text_nonorm(ground_truths[\"__ALL__\"]))\n",
    "    elif doc_format == \"json\":\n",
    "        doc_lengths_normalized, doc_lengths_nonorm, total_doc_len_normalized, total_doc_len_nonorm = {}, {}, 0, 0\n",
    "        for doc, json_data in ground_truths.items():\n",
    "\n",
    "            # Loop over each entry in json object array\n",
    "            for entry in json_data[\"entries\"]:\n",
    "\n",
    "                # Loop over each field's value in the entry\n",
    "                for text in entry.values():\n",
    "                    doc_lengths_normalized[doc] = doc_lengths_normalized.get(doc, 0) + len(entry)\n",
    "                    doc_lengths_nonorm[doc] = doc_lengths_nonorm.get(doc, 0) + len(entry)\n",
    "            \n",
    "            # Add up the totals as we go along with doc_lengths_normalized etc.\n",
    "            total_doc_len_normalized += doc_lengths_normalized[doc]\n",
    "            total_doc_len_nonorm += doc_lengths_nonorm[doc]\n",
    "\n",
    "    # -> Gather each transcribed document and put into dict:\n",
    "\n",
    "    # Structure: results[model][doc]\n",
    "    results = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        logger.info(\"Collecting results for model: %s\", model)\n",
    "        model_path = os.path.join(output_dir, model_type, model)\n",
    "        results[model_type] = results.get(model_type, {})\n",
    "        results[model_type][model], results[model_type][model][\"__ALL__\"] = get_docs(model_path, doc_names, doc_format, name_has_prefix=False)\n",
    "        logger.info(\"Collected results for model_type: %s, model: %s\", model_type, model)\n",
    "\n",
    "    # ===============\n",
    "    # Compute metrics\n",
    "    # ===============\n",
    "\n",
    "    normalized_results_data = {}\n",
    "    nonorm_results_data = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        normalized_results_data[model_type] = normalized_results_data.get(model_type, {})\n",
    "        normalized_results_data[model_type][model] = normalized_results_data[model_type].get(model, {})\n",
    "        nonorm_results_data[model_type] = nonorm_results_data.get(model_type, {})\n",
    "        nonorm_results_data[model_type][model] = nonorm_results_data[model_type].get(model, {})\n",
    "\n",
    "        logger.info(\"Computing metrics for model_type: %s, model: %s\", model_type, model)\n",
    "        for doc in doc_names:\n",
    "            logger.info(\"Computing metrics for document: %s\", doc)\n",
    "            normalized_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], doc_format, normalized=True\n",
    "            )\n",
    "            nonorm_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], doc_format, normalized=False\n",
    "            )\n",
    "\n",
    "        normalized_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], doc_format, normalized=True\n",
    "        )\n",
    "        nonorm_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], doc_format, normalized=False\n",
    "        )\n",
    "\n",
    "    # Compute metrics separately for __ALL__]\n",
    "\n",
    "    # ====================\n",
    "    # Put metrics in table\n",
    "    # ====================\n",
    "\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    results_base_dir = root_dir / \"benchmarking-results\" / f\"{doc_format}-accuracy\"\n",
    "\n",
    "    # Create different results directory for each model type\n",
    "    for model_type, _ in all_models:\n",
    "        results_dir = results_base_dir / model_type\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        normalized_df = build_dataframe(\n",
    "            f\"normalized_{time}\",\n",
    "            doc_names,\n",
    "            normalized_results_data[model_type],\n",
    "            doc_lengths_normalized,\n",
    "            total_doc_len_normalized,\n",
    "        )\n",
    "        nonorm_df = build_dataframe(\n",
    "            f\"nonorm_{time}\",\n",
    "            doc_names,\n",
    "            nonorm_results_data[model_type],\n",
    "            doc_lengths_nonorm,\n",
    "            total_doc_len_nonorm,\n",
    "        )\n",
    "\n",
    "        # ============\n",
    "        # Save results\n",
    "        # ============\n",
    "\n",
    "        # # Default save to project_root/benchmarking-results/txt-accuracy\n",
    "        # results_path = os.path.join(project_root, \"benchmarking-results\", \"txt-accuracy\")\n",
    "        # if not os.path.exists(results_path):\n",
    "        #     os.makedirs(results_path)\n",
    "        normalized_df.to_csv(os.path.join(str(results_dir), f\"normalized_{time}.csv\"))\n",
    "        nonorm_df.to_csv(os.path.join(str(results_dir), f\"nonorm_{time}.csv\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
