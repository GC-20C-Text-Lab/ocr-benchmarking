{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4432e892",
   "metadata": {},
   "source": [
    "# OCR-mLLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd231b04",
   "metadata": {},
   "source": [
    "Before running this code you will need to set up your OpenAI & Gemini API keys. Here's how I did it:\n",
    "\n",
    "1. Create a new file in your root directory called `.env` (no prefix)\n",
    "2. Store your API keys with the following names: OPENAI_API_KEY, ANTHROPIC_API_KEY, and GOOGLE_API_KEY\n",
    "3. Create a virtual environment by typing the following commands into your terminal:\n",
    "    - ```python3 -m venv .venv```\n",
    "    - ```source .venv/bin/activate```\n",
    "    - ```pip install -r requirements.txt```\n",
    "4. After running the pipeline, type ```deactivate``` in your terminal to make everything go back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7cf4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002663b1",
   "metadata": {},
   "source": [
    "### a. Run this cell to ensure you have all the necessary directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3540d",
   "metadata": {},
   "source": [
    "Before running the cell make sure you have an images folder in your root directory to feed the images into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "549ec682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Get the root directory of the project\n",
    "root_dir = Path.cwd().parent.parent\n",
    "\n",
    "# Get the user's path for the images folder assuming all images are stored here in .png format\n",
    "source_dir = root_dir / \"images\"\n",
    "\n",
    "# Get the user's path for the output folder, create one if it doesn't exist\n",
    "txt_output_dir = root_dir / \"results\"\n",
    "txt_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# bm_output_dir = root_dir / \"benchmarking-results\"/ \"txt-accuracy\"\n",
    "# bm_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# llm_array = [\"gpt-4o\", \"gemini-2.5-flash\", \"claude-4-sonnet\"]\n",
    "llm_array = [\"gpt-4o\", \"gemini-2.0-flash\"]\n",
    "\n",
    "def make_llm_dirs(llm_array, target_dir):\n",
    "    for llm in llm_array:\n",
    "        dir = target_dir / \"ocr_img2txt\"\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "        dir = target_dir / \"llm_img2txt\" / llm\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "        dir = target_dir / \"ocr_llm_img2txt\" / llm\n",
    "        dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "make_llm_dirs(llm_array, txt_output_dir)\n",
    "# make_llm_dirs(llm_array, bm_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859c660",
   "metadata": {},
   "source": [
    "### b. Setup API keys & image encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7ae833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "gpt_client = OpenAI(api_key=openai_api_key)\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "claude_client = Anthropic(api_key=anthropic_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a5773",
   "metadata": {},
   "source": [
    "### c. Get image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fed71bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all filenames in images directory into the `filenames` array with the ENTIRE filepath\n",
    "img_filepaths = []\n",
    "for path in source_dir.iterdir():\n",
    "  if path.suffix.lower() == \".png\" and path.is_file():\n",
    "    img_filepaths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702eec5-e52d-4b51-8907-f593204a1b76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Run pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95e34ea1-f6ae-4de7-9887-764da7178f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files from ocr-benchmarking/images folder & write to results folder\n",
    "for path in img_filepaths:\n",
    "    file_name = txt_output_dir / \"ocr_img2txt\" / path.stem\n",
    "    file_name = str(file_name) + \".txt\"\n",
    "    \n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(pytesseract.image_to_string(Image.open(str(path)))) # TODO: Change config as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df39757",
   "metadata": {},
   "source": [
    "## 3. Prepare the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "045337cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_ocr_llm = \"\"\"\n",
    "You are a text correction assistant. Your task is to clean up and correct errors from raw OCR output.\n",
    "The text may contain misrecognized characters, broken words, or incorrect formatting.\n",
    "Carefully read the provided OCR output and produce a corrected version that is grammatically accurate \n",
    "and as faithful to the original content as possible. Because this is a historical document, try to \n",
    "preserve archaic spelling or formatting where clearly intended. Only correct obvious OCR errors.\n",
    "Put the dates associated with each entry at the end of the line.\n",
    "\n",
    "Input (Raw OCR Text):\n",
    "{input}\n",
    "\"\"\"\n",
    "input = \"\"\n",
    "with open(\"/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/results/ocr_img2txt/kbaa-p096.txt\", 'r') as file:\n",
    "    input += file.read()\n",
    "\n",
    "prompt_ocr_llm = prompt_template_ocr_llm.format(input=input).strip()\n",
    "\n",
    "prompt_llm = \"\"\"\n",
    "You are an expert historian. Your task is to transcribe the provided image into text. The image\n",
    "is a 20th century bibliographic entry. Because this is a historical document, try to preserve \n",
    "archaic spelling or formatting where clearly intended. Put the dates associated with each entry at the end of the line.\n",
    "Return the text only, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# From the provided image, give me the first word and nothing else\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58cb0c-aeb8-47cc-9528-26bc3a802984",
   "metadata": {},
   "source": [
    "## 4. Send to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06535758",
   "metadata": {},
   "source": [
    "### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "163080e4-5134-407c-9cdd-7a89141e1632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:04:42 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    base64_image = encode_image(path)\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_ocr_llm\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    with open(txt_output_dir / \"ocr_llm_img2txt\" / \"gpt-4o\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "        file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cafe77",
   "metadata": {},
   "source": [
    "### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18c2eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:05:08 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    base64_image = encode_image(path)\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt_llm\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    with open(txt_output_dir / \"llm_img2txt\" / \"gpt-4o\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "        file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ecae5",
   "metadata": {},
   "source": [
    "## 5. Send to Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10c212",
   "metadata": {},
   "source": [
    "### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9e5d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:05:08 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:05:08 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH89bHeN0-vA-xzs2K_EGUYmGBXEGUqZ-TzID8lIRuuIZ5uv_yAciVJ4QlyDuV6hzZ8cjQZLSwSf8qPnMUXfMtLoxSxcspz6tzUhrZBAqjyo&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:05:12 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH89bHeN0-vA-xzs2K_EGUYmGBXEGUqZ-TzID8lIRuuIZ5uv_yAciVJ4QlyDuV6hzZ8cjQZLSwSf8qPnMUXfMtLoxSxcspz6tzUhrZBAqjyo&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:05:12 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-06-23 18:05:22 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:05:22 [INFO] AFC remote call 1 is done.\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    my_file = gemini_client.files.upload(file=path)\n",
    "\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=[\n",
    "            prompt_ocr_llm,\n",
    "            my_file\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(txt_output_dir / \"ocr_llm_img2txt\" / \"gemini-2.0-flash\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d3100",
   "metadata": {},
   "source": [
    "### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5d3d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:09:12 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:09:12 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8-3KU37dFDba1mFKIvOfc5RYRqFx9d62ZmHwTuLZYDqHsclMmASOT5Yrxi_PKCGIThqeKhARRBqdZIO2RFRET1a82VvUpf2BZRgTsaFYHE&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:09:15 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8-3KU37dFDba1mFKIvOfc5RYRqFx9d62ZmHwTuLZYDqHsclMmASOT5Yrxi_PKCGIThqeKhARRBqdZIO2RFRET1a82VvUpf2BZRgTsaFYHE&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:09:15 [INFO] AFC is enabled with max remote calls: 10.\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] AFC remote call 1 is done.\n"
     ]
    }
   ],
   "source": [
    "for path in img_filepaths:\n",
    "    my_file = gemini_client.files.upload(file=path)\n",
    "\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=[\n",
    "            prompt_llm,\n",
    "            my_file\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(txt_output_dir / \"llm_img2txt\" / \"gemini-2.0-flash\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "        file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d06c34",
   "metadata": {},
   "source": [
    "## 6. Send to Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0869c81",
   "metadata": {},
   "source": [
    "### a. OCR-LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8651e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in img_filepaths:\n",
    "#     base64_image = encode_image(path)\n",
    "\n",
    "#     response = claude_client.messages.create(\n",
    "#         model='claude-opus-4-20250514',\n",
    "#         temperature=0,\n",
    "#         max_tokens=10,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\", \n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt_ocr_llm\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": \"image/png\",\n",
    "#                             \"data\": base64_image\n",
    "#                         }\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#             ]\n",
    "#     )\n",
    "#     print(response)\n",
    "\n",
    "#     with open(txt_output_dir / \"ocr_llm_img2txt\" / \"claude-4-sonnet\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "#         file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610ccd9",
   "metadata": {},
   "source": [
    "### b. LLM call (without OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d497c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in img_filepaths:\n",
    "#     base64_image = encode_image(path)\n",
    "\n",
    "#     response = claude_client.messages.create(\n",
    "#         model='claude-opus-4-20250514',\n",
    "#         temperature=0,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\", \n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt_llm\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": \"image/png\",\n",
    "#                             \"data\": base64_image\n",
    "#                         }\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#             ]\n",
    "#     )\n",
    "\n",
    "#     with open(txt_output_dir / \"llm_img2txt\" / \"claude-4-sonnet\" / Path(path.stem + \".txt\"), 'w') as file:\n",
    "#         file.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e20262",
   "metadata": {},
   "source": [
    "## 7. Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77c21953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Script directory: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/src/workflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Project root: /Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Found ground-truth txt files: ['/Users/muhammadkhalid/Desktop/map2025/ocr-benchmarking/ground-truth/kbaa-p096.txt']\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Found file names: ['kbaa-p096']\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Models found: [('llm_img2txt', 'gemini-2.0-flash'), ('ocr_llm_img2txt', 'gemini-2.0-flash'), ('llm_img2txt', 'gpt-4o'), ('ocr_llm_img2txt', 'gpt-4o')]\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collecting results for model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collected results for model_type: llm_img2txt, model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collecting results for model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collected results for model_type: ocr_llm_img2txt, model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collecting results for model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collected results for model_type: llm_img2txt, model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collecting results for model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Collected results for model_type: ocr_llm_img2txt, model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for model_type: llm_img2txt, model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for document: kbaa-p096\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for model_type: ocr_llm_img2txt, model: gemini-2.0-flash\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for document: kbaa-p096\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for model_type: llm_img2txt, model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for document: kbaa-p096\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for model_type: ocr_llm_img2txt, model: gpt-4o\n",
      "[file retrieval] 2025-06-23 18:09:24 [INFO] Computing metrics for document: kbaa-p096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dist_char': 223, 'cer': 0.06630984240261671, 'wer': 0.37478991596638656, 'token_sort_ratio': 99.18239928645755}\n",
      "{'dist_char': 317, 'cer': 0.08635249250885317, 'wer': 0.5097402597402597, 'token_sort_ratio': 97.13155291790306}\n",
      "{'dist_char': 219, 'cer': 0.06512042818911686, 'wer': 0.3680672268907563, 'token_sort_ratio': 99.24231169217056}\n",
      "{'dist_char': 312, 'cer': 0.08499046581312994, 'wer': 0.5048701298701299, 'token_sort_ratio': 97.18886848424918}\n",
      "{'dist_char': 63, 'cer': 0.01873327386262266, 'wer': 0.10588235294117647, 'token_sort_ratio': 98.68302903322359}\n",
      "{'dist_char': 77, 'cer': 0.02097521111413784, 'wer': 0.12012987012987013, 'token_sort_ratio': 98.27807030027039}\n",
      "{'dist_char': 37, 'cer': 0.011002081474873625, 'wer': 0.06218487394957983, 'token_sort_ratio': 99.35945180992105}\n",
      "{'dist_char': 58, 'cer': 0.01579950967038954, 'wer': 0.09415584415584416, 'token_sort_ratio': 98.65610411656527}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from benchmarking.txt_accuracy import clean_text_normalized, clean_text_nonorm, compute_metrics, build_dataframe, get_doc_names, get_all_models, get_docs\n",
    "from venv import logger\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Prerequisites:\n",
    "    - Ground truth text files located at `project_root/ground-truth/txt/kbaa-p#xyz.txt`\n",
    "    - LLM/OCR transcribed files located at:\n",
    "        - for LLM transcriptions: `project_root/results/llm_img2txt/<MODEL-NAME>/kbaa-p#xyz.txt`\n",
    "        - for OCR transcriptions: `project_root/results/ocr_img2txt/<MODEL-NAME>/kbaa-p#xyz.txt`\n",
    "\n",
    "    The main function will:\n",
    "    - Gather all ground truth text files\n",
    "    - For each ground truth text file and for each LLM/OCR model, gather the corresponding transcription\n",
    "    - Clean all the text files (normalized and not normalized)\n",
    "    - Compute metrics for each file and model\n",
    "    - Save results in two CSV files (one for normalized, one for non-normalized)\n",
    "        - Results are saved in `project_root/benchmarking-results/txt-accuracy`\n",
    "    \"\"\"\n",
    "\n",
    "    # =============\n",
    "    # Preliminaries\n",
    "    # =============\n",
    "\n",
    "    # args = parse_arguments()\n",
    "\n",
    "    script_dir = str(Path.cwd())\n",
    "    project_root = str(root_dir)\n",
    "    logger.info(\"Script directory: %s\", script_dir)\n",
    "    logger.info(\"Project root: %s\", project_root)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth_dir = root_dir / \"ground-truth\"\n",
    "    doc_names = get_doc_names(ground_truth_dir)\n",
    "\n",
    "    # results/ paths\n",
    "    all_models = get_all_models(\n",
    "        os.path.join(project_root, \"results\", \"llm_img2txt\"),\n",
    "        os.path.join(project_root, \"results\", \"ocr_img2txt\"),\n",
    "        os.path.join(project_root, \"results\", \"ocr_llm_img2txt\"),\n",
    "    )\n",
    "    logger.info(f\"Models found: {all_models}\")\n",
    "\n",
    "    # ===========\n",
    "    # Gather files\n",
    "    # ===========\n",
    "\n",
    "    # -> Gather ground truths and put into dict:\n",
    "\n",
    "    ground_truths, ground_truths[\"__ALL__\"] = get_docs(ground_truth_dir, doc_names)\n",
    "    doc_lengths_normalized = {\n",
    "        doc: len(clean_text_normalized(text)) for doc, text in ground_truths.items()\n",
    "    }\n",
    "    doc_lengths_nonorm = {\n",
    "        doc: len(clean_text_nonorm(text)) for doc, text in ground_truths.items()\n",
    "    }\n",
    "    total_doc_len_normalized = len(clean_text_normalized(ground_truths[\"__ALL__\"]))\n",
    "    total_doc_len_nonorm = len(clean_text_nonorm(ground_truths[\"__ALL__\"]))\n",
    "\n",
    "    # -> Gather each transcribed document and put into dict:\n",
    "\n",
    "    # Structure: results[model][doc]\n",
    "    results = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        logger.info(\"Collecting results for model: %s\", model)\n",
    "        model_path = os.path.join(project_root, \"results\", model_type, model)\n",
    "        results[model_type] = results.get(model_type, {})\n",
    "        results[model_type][model], results[model_type][model][\"__ALL__\"] = get_docs(model_path, doc_names)\n",
    "            \n",
    "        # logger.info(\"Collected results for model: %s\", list(results[model].keys()))\n",
    "        logger.info(\"Collected results for model_type: %s, model: %s\", model_type, model)\n",
    "\n",
    "    # ===============\n",
    "    # Compute metrics\n",
    "    # ===============\n",
    "\n",
    "    normalized_results_data = {}\n",
    "    nonorm_results_data = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        normalized_results_data[model_type] = normalized_results_data.get(model_type, {})\n",
    "        normalized_results_data[model_type][model] = normalized_results_data[model_type].get(model, {})\n",
    "        nonorm_results_data[model_type] = nonorm_results_data.get(model_type, {})\n",
    "        nonorm_results_data[model_type][model] = nonorm_results_data[model_type].get(model, {})\n",
    "\n",
    "        logger.info(\"Computing metrics for model_type: %s, model: %s\", model_type, model)\n",
    "        for doc in doc_names:\n",
    "            logger.info(\"Computing metrics for document: %s\", doc)\n",
    "            normalized_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], normalized=True\n",
    "            )\n",
    "            nonorm_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], normalized=False\n",
    "            )\n",
    "            print(normalized_results_data[model_type][model][doc])\n",
    "            print(nonorm_results_data[model_type][model][doc])\n",
    "\n",
    "        normalized_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], normalized=True\n",
    "        )\n",
    "        nonorm_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], normalized=False\n",
    "        )\n",
    "\n",
    "    # Compute metrics separately for __ALL__]\n",
    "\n",
    "    # ====================\n",
    "    # Put metrics in table\n",
    "    # ====================\n",
    "\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    results_base_dir = root_dir / \"benchmarking-results\" / \"txt-accuracy\"\n",
    "\n",
    "    # Create different results directory for each model type\n",
    "    for model_type, _ in all_models:\n",
    "        results_dir = results_base_dir / model_type\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        normalized_df = build_dataframe(\n",
    "            f\"normalized_{time}\",\n",
    "            doc_names,\n",
    "            normalized_results_data[model_type],\n",
    "            doc_lengths_normalized,\n",
    "            total_doc_len_normalized,\n",
    "        )\n",
    "        nonorm_df = build_dataframe(\n",
    "            f\"nonorm_{time}\",\n",
    "            doc_names,\n",
    "            nonorm_results_data[model_type],\n",
    "            doc_lengths_nonorm,\n",
    "            total_doc_len_nonorm,\n",
    "        )\n",
    "\n",
    "        # ============\n",
    "        # Save results\n",
    "        # ============\n",
    "\n",
    "        # # Default save to project_root/benchmarking-results/txt-accuracy\n",
    "        # results_path = os.path.join(project_root, \"benchmarking-results\", \"txt-accuracy\")\n",
    "        # if not os.path.exists(results_path):\n",
    "        #     os.makedirs(results_path)\n",
    "        normalized_df.to_csv(os.path.join(str(results_dir), f\"normalized_{time}.csv\"))\n",
    "        nonorm_df.to_csv(os.path.join(str(results_dir), f\"nonorm_{time}.csv\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a2cae",
   "metadata": {},
   "source": [
    "## Don't use the cell below, this is just for my use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad9ab0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# from benchmarking.txt_accuracy import clean_text_normalized\n",
    "\n",
    "# gt_dir = root_dir / \"ground-truth\"\n",
    "# ocr_dir = root_dir / \"results\" / \"ocr_img2txt\"\n",
    "# llm_dir = root_dir / \"results\" / \"llm_img2txt\" / \"gpt-4o\"\n",
    "# ocr_llm_dir = root_dir / \"results\" / \"ocr_llm_img2txt\" / \"gpt-4o\"\n",
    "\n",
    "# ocr_llm_filepaths = []\n",
    "# for path in ocr_llm_dir.iterdir():\n",
    "#   if path.is_file():\n",
    "#     if \"3\" in path.stem:\n",
    "#         continue\n",
    "#     ocr_llm_filepaths.append(path)\n",
    "\n",
    "# ocr_filepaths = []\n",
    "# for path in ocr_dir.iterdir():\n",
    "#   if path.is_file():\n",
    "#     if \"3\" in path.stem:\n",
    "#         continue\n",
    "#     ocr_filepaths.append(path)\n",
    "\n",
    "# llm_filepaths = []\n",
    "# for path in llm_dir.iterdir():\n",
    "#   if path.is_file():\n",
    "#     if \"3\" in path.stem:\n",
    "#         continue\n",
    "#     llm_filepaths.append(path)\n",
    "\n",
    "# gt_filepaths = []\n",
    "# for path in gt_dir.iterdir():\n",
    "#   if path.is_file():\n",
    "#     gt_filepaths.append(path)\n",
    "\n",
    "# # if len(ocr_filepaths) != len(gt_filepaths):\n",
    "# #     raise ValueError(\"Number of OCR files and GT files do not match\")\n",
    "\n",
    "# for ocr_llm_path, llm_path, gt_path in zip(ocr_llm_filepaths, llm_filepaths, gt_filepaths):\n",
    "#     with open(ocr_llm_path, 'r') as file:\n",
    "#         ocr_llm_text = clean_text_normalized(file.read())\n",
    "#     with open(llm_path, 'r') as file:\n",
    "#         llm_text = clean_text_normalized(file.read())\n",
    "#     with open(gt_path, 'r') as file:\n",
    "#         gt_text = clean_text_normalized(file.read())\n",
    "# print(llm_text)\n",
    "# print(gt_text)\n",
    "\n",
    "# for i in range(len(ocr_llm_text)):\n",
    "#     if ocr_llm_text[i] != gt_text[i]:\n",
    "#         print(f\"Mismatch at index {i}\")\n",
    "#         print(f\"OCR: {ocr_llm_text[i]}\")\n",
    "#         print(f\"GT: {gt_text[i]}\")\n",
    "#         break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
