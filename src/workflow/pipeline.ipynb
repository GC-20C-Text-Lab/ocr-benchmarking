{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4432e892",
   "metadata": {},
   "source": [
    "# OCR-mLLM Pipeline Documentation\n",
    "\n",
    "This notebook implements a pipeline for processing images through OCR (Optical Character Recognition) and Large Language Models (LLMs). The pipeline supports multiple processing paths:\n",
    "\n",
    "1. Raw OCR extraction (output in plain text): Pages are processed through pytesseract to generate raw OCR text - without mLLM intervention.\n",
    "2. Direct image transcription (output in plain text): The original PNG is processed by the mLLM.\n",
    "3. OCR post-correction (output in plain text): Raw OCR text and the original PNG are both fed into the mLLM with the OCR output clearly labelled in the prompt.\n",
    "4. JSON from corrected text (output in JSON): OCR post-corrected text is used as input to generate structured JSON output.\n",
    "5. Direct JSON from image (output in JSON): The original PNG is processed directly to produce structured JSON data.\n",
    "\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "The pipeline creates and uses the following directory structure:\n",
    "\n",
    "```\n",
    "project_root/\n",
    "├── data/\n",
    "│   └── pngs/            # Source images in PNG format\n",
    "├── results/\n",
    "│   ├── txt/            # Text outputs\n",
    "│   │   ├── ocr-img2txt/pytesseract/     # Raw OCR output\n",
    "│   │   ├── llm-img2txt/{model}/         # Direct LLM image transcription\n",
    "│   │   └── ocr-llm-img2txt/{model}/     # LLM-refined OCR output\n",
    "│   └── json/           # JSON outputs\n",
    "│       ├── llm-img2json/{model}/        # Direct image to JSON via LLM\n",
    "│       └── llm-txt2json/{model}/        # Text to JSON via LLM\n",
    "└── benchmarking-results/\n",
    "    ├── txt-accuracy/   # Text accuracy metrics\n",
    "    └── json-accuracy/  # JSON accuracy metrics\n",
    "```\n",
    "\n",
    "## Supported Models\n",
    "\n",
    "The pipeline currently supports:\n",
    "- OpenAI GPT-4V (gpt-4o)\n",
    "- Google Gemini Pro Vision (gemini-2.5-flash)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. Create your virtual environment: (optional)\n",
    "   ```bash\n",
    "   python3 -m venv .venv\n",
    "   source .venv/bin/activate\n",
    "   ```\n",
    "   After running the notebook type `deactivate` in the terminal to stop your virtual environment\n",
    "\n",
    "   With Anaconda:\n",
    "   ```bash\n",
    "   conda env create --file=environment.yaml\n",
    "   conda activate ocr-benchmarking\n",
    "   ```\n",
    "\n",
    "2. Install required packages (if not using Anaconda):\n",
    "   ```bash\n",
    "   pip install -r config/requirements.txt\n",
    "   ```\n",
    "\n",
    "2. Set up API keys in a `.env` file:\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_key_here\n",
    "   GOOGLE_API_KEY=your_key_here\n",
    "   ```\n",
    "\n",
    "3. Place source images in PNG format in the `data/pngs/` directory\n",
    "\n",
    "4. For Windows users: Install Tesseract OCR and set the path in the notebook\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "The notebook is organized into several key sections:\n",
    "\n",
    "1. Setup & Initialization\n",
    "2. OCR Processing (Tesseract)\n",
    "3. LLM Processing (GPT-4V, Gemini)\n",
    "4. JSON Conversion\n",
    "5. Benchmarking & Metrics\n",
    "\n",
    "Each section can be run independently, though they build on each other in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ab3ba",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup & Initialization\n",
    "\n",
    "### Directory Setup\n",
    "\n",
    "The first code cell handles:\n",
    "- Setting up project paths and directory structure\n",
    "- Creating necessary output directories\n",
    "- Configuring model names and paths\n",
    "\n",
    "The `make_llm_dirs()` function creates standardized directory structures for each LLM model's outputs, supporting both text and JSON formats. Key components include:\n",
    "\n",
    "```python\n",
    "# Directory structure\n",
    "source_dir = root_dir / \"data\" / \"pngs\"              # Input images\n",
    "txt_output_dir = root_dir / \"results\" / \"txt\"        # Text outputs\n",
    "json_output_dir = root_dir / \"results\" / \"json\"      # JSON outputs\n",
    "bm_txt_output_dir = root_dir / \"benchmarking-results\"/ \"txt-accuracy\"  # Benchmarking results\n",
    "```\n",
    "\n",
    "The `make_llm_dirs()` function ensures each model has its own organized output directories for different processing stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df793",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### API Client Setup\n",
    "\n",
    "The second code cell initializes API clients and utilities:\n",
    "\n",
    "1. **API Clients**:\n",
    "   - OpenAI client for GPT-4V\n",
    "   - Google client for Gemini Pro Vision\n",
    "\n",
    "2. **Environment Variables**:\n",
    "   API keys are loaded from environment variables:\n",
    "   ```python\n",
    "   openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "   # Google API key is loaded directly in client initialization\n",
    "   ```\n",
    "\n",
    "Make sure all required API keys are set in your `.env` file before running this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14072c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Image File Loading\n",
    "\n",
    "The third code cell prepares the input data. It:\n",
    "   - Scans the `data/pngs/` directory\n",
    "   - Filters for PNG files only\n",
    "   - Stores full file paths for processing\n",
    "\n",
    "This list of image paths (`img_filepaths`) will be used by subsequent processing steps in the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7cf4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002663b1",
   "metadata": {},
   "source": [
    "### a. Run this cell to ensure you have all the necessary directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3540d",
   "metadata": {},
   "source": [
    "Before running the cell make sure you have an images folder in your root directory to feed the images into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ec682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "from venv import logger\n",
    "from json_creation import *\n",
    "from google.genai import types\n",
    "sys.path.append('../')\n",
    "\n",
    "# Get the root directory of the project\n",
    "root_dir = Path.cwd().parent.parent\n",
    "\n",
    "sys.path.append(Path(str(root_dir / \"tools\")))\n",
    "from tools.file_retrieval import *\n",
    "                \n",
    "# Get the user's path for the images folder assuming all images are stored here in .png format\n",
    "source_dir = root_dir / \"data\" / \"pngs\"\n",
    "txt_source_dir = root_dir / \"results\" / \"txt\" / \"ocr-llm-img2txt\"\n",
    "\n",
    "# Get the path for the output folders, create one if it doesn't exist\n",
    "txt_output_dir = root_dir / \"results\" / \"txt\" # txt output\n",
    "txt_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_output_dir = root_dir / \"results\" / \"json\" # json output\n",
    "json_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bm_txt_output_dir = root_dir / \"benchmarking-results\"/ f\"txt-accuracy\" # benchmarking text output\n",
    "bm_txt_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bm_json_output_dir = root_dir / \"benchmarking-results\"/ f\"json-accuracy\" # benchmarking json output\n",
    "bm_json_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "llms = {\"openai\": \"gpt-4o\", \"google\": \"gemini-2.5-flash\"}\n",
    "\n",
    "# Create relevant directories for mLLM & OCR outputs\n",
    "def make_llm_dirs(llms, target_dir, doc_format):\n",
    "    for llm in llms.values():\n",
    "        if doc_format == \"txt\":\n",
    "            dir = target_dir / f\"ocr-img2txt\" / \"pytesseract\"\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"llm-img2txt\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"ocr-llm-img2txt\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            dir = target_dir / f\"llm-img2json\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "            dir = target_dir / f\"llm-txt2json\" / llm\n",
    "            dir.mkdir(parents=True, exist_ok=True)\n",
    "make_llm_dirs(llms, txt_output_dir, \"txt\")\n",
    "make_llm_dirs(llms, json_output_dir, \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859c660",
   "metadata": {},
   "source": [
    "### b. Setup API keys & image encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb7ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "import base64\n",
    "from json_creation import *\n",
    "from txt_creation import *\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "gpt_client = OpenAI(api_key=openai_api_key)\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "claude_client = Anthropic(api_key=anthropic_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a5773",
   "metadata": {},
   "source": [
    "### c. Get image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed71bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all filenames in images directory into the `filenames` array with the ENTIRE filepath\n",
    "img_filepaths = []\n",
    "ocr_output_filepaths = []\n",
    "\n",
    "for path in source_dir.iterdir():\n",
    "    if path.suffix.lower() == \".png\" and path.is_file():\n",
    "      img_filepaths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702eec5-e52d-4b51-8907-f593204a1b76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Run pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c314b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows users should run this cell, inserting their path to Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e34ea1-f6ae-4de7-9887-764da7178f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files from ocr-benchmarking/images folder & write to results folder\n",
    "for path in img_filepaths:\n",
    "    file_name = txt_output_dir / \"ocr-img2txt\" / \"pytesseract\" / path.stem\n",
    "    file_name = str(file_name) + \".txt\"\n",
    "    \n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(pytesseract.image_to_string(Image.open(str(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58cb0c-aeb8-47cc-9528-26bc3a802984",
   "metadata": {},
   "source": [
    "## 3. OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d64bb8",
   "metadata": {},
   "source": [
    "### (i) Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa37e9",
   "metadata": {},
   "source": [
    "#### a. OCR-LLM (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch ocr output files\n",
    "ocr_output_dir = txt_output_dir/\"ocr-img2txt\"/\"pytesseract\"\n",
    "ocr_output_filepaths = get_paths(ocr_output_dir, \"txt\")\n",
    "\n",
    "# Run the async processes\n",
    "await process_double_async(img_filepaths, ocr_output_filepaths, txt_output_dir/\"ocr-llm-img2txt\", openai_img_txt2txt_async, llms['openai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267fdde9",
   "metadata": {},
   "source": [
    "#### b. LLM (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544747",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_single_async(img_filepaths, txt_output_dir/\"llm-img2txt\", openai_img2txt_async, llms['openai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5692d2",
   "metadata": {},
   "source": [
    "### (ii) JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ed85e",
   "metadata": {},
   "source": [
    "#### a. Image to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfa470",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_json_async(img_filepaths, json_output_dir/\"llm-img2json\", openai_img2json_async, llms['openai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ae2c9",
   "metadata": {},
   "source": [
    "#### b. Text to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = txt_source_dir / llms['openai'] # where to look for ocr-llm-img2txt output\n",
    "# Get the text paths from ocr-llm-img2txt/gpt-4o directory\n",
    "txt_filepaths = get_paths(dir, \"txt\")\n",
    "\n",
    "# Call the main function that concurrently runs relevant async function\n",
    "await process_json_async(txt_filepaths, json_output_dir/\"llm-txt2json\", openai_txt2json_async, llms['openai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ecae5",
   "metadata": {},
   "source": [
    "## 4. Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fb167",
   "metadata": {},
   "source": [
    "### (i) Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29d8b3",
   "metadata": {},
   "source": [
    "#### a. LLM call (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17594637",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_single_async(img_filepaths, txt_output_dir/\"llm-img2txt\", gemini_img2txt_async, llms['google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5af92",
   "metadata": {},
   "source": [
    "#### b. OCR-LLM (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch ocr output files\n",
    "ocr_output_dir = txt_output_dir/\"ocr-img2txt\"/\"pytesseract\"\n",
    "ocr_output_filepaths = get_paths(ocr_output_dir, \"txt\")\n",
    "\n",
    "# Run the async processes\n",
    "await process_double_async(img_filepaths, ocr_output_filepaths, txt_output_dir/\"ocr-llm-img2txt\", gemini_img_txt2txt_async, llms['google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cec32",
   "metadata": {},
   "source": [
    "### (ii) JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e600840",
   "metadata": {},
   "source": [
    "#### a. Image to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b03c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_json_async(img_filepaths, json_output_dir/\"llm-img2json\", gemini_img2json_async, llms['google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d6b4d",
   "metadata": {},
   "source": [
    "#### b. Text to JSON (Async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = txt_source_dir / llms['google'] # where to look for ocr-llm-img2txt output\n",
    "\n",
    "# Get the text paths from ocr-llm-img2txt/gpt-4o directory\n",
    "txt_filepaths = get_paths(dir, \"txt\")\n",
    "\n",
    "# Call the main function that concurrently runs relevant async function\n",
    "await process_json_async(txt_filepaths, json_output_dir/\"llm-txt2json\", gemini_txt2json_async, llms['google'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e20262",
   "metadata": {},
   "source": [
    "## 5. Benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3596a",
   "metadata": {},
   "source": [
    "a. Text accuracy benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c21953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().\n",
    "parent))\n",
    "from benchmarking.txt_accuracy import clean_text_normalized, clean_text_nonorm, compute_metrics, build_dataframe\n",
    "from tools.file_retrieval import get_doc_names, get_docs, get_all_models\n",
    "from datetime import datetime\n",
    "from venv import logger\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Prerequisites:\n",
    "    - Ground truth text files located at `project_root/ground-truth/txt/kbaa-pxyz.txt`\n",
    "    - LLM/OCR transcribed files located at:\n",
    "        - for LLM transcriptions: `project_root/results/llm_img2txt/<MODEL-NAME>/kbaa-pxyz.txt`\n",
    "        - for OCR transcriptions: `project_root/results/ocr_img2txt/<MODEL-NAME>/kbaa-pxyz.txt`\n",
    "\n",
    "    The main function will:\n",
    "    - Gather all ground truth text files\n",
    "    - For each ground truth text file and for each LLM/OCR model, gather the corresponding transcription\n",
    "    - Clean all the text files (normalized and not normalized)\n",
    "    - Compute metrics for each file and model\n",
    "    - Save results in two CSV files (one for normalized, one for non-normalized)\n",
    "        - Results are saved in `project_root/benchmarking-results/txt-accuracy`\n",
    "    \"\"\"\n",
    "\n",
    "    # =============\n",
    "    # Preliminaries\n",
    "    # =============\n",
    "\n",
    "    # args = parse_arguments()\n",
    "\n",
    "    script_dir = str(Path.cwd())\n",
    "    project_root = str(root_dir)\n",
    "    logger.info(\"Script directory: %s\", script_dir)\n",
    "    logger.info(\"Project root: %s\", project_root)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth_dir = root_dir / \"data\" / \"ground-truth\" / \"txt\"\n",
    "    doc_names = get_doc_names(ground_truth_dir, \"txt\", keep_prefix=False)\n",
    "\n",
    "    # results/ paths\n",
    "    all_models = get_all_models(\n",
    "        \"txt\",\n",
    "        os.path.join(txt_output_dir, f\"llm-img2txt\"),\n",
    "        os.path.join(txt_output_dir, \"ocr-img2txt\"),\n",
    "        os.path.join(txt_output_dir, f\"ocr-llm-img2txt\"),\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Models found: {all_models}\")\n",
    "\n",
    "    # ===========\n",
    "    # Gather files\n",
    "    # ===========\n",
    "\n",
    "    # -> Gather ground truths and put into dict:\n",
    "    ground_truths, all_texts = get_docs(ground_truth_dir, doc_names, \"txt\", name_has_prefix=True)\n",
    "    ground_truths[\"__ALL__\"] = all_texts\n",
    "\n",
    "    doc_lengths_normalized = {\n",
    "        doc: len(clean_text_normalized(text)) for doc, text in ground_truths.items()\n",
    "    }\n",
    "    doc_lengths_nonorm = {\n",
    "        doc: len(clean_text_nonorm(text)) for doc, text in ground_truths.items()\n",
    "    }\n",
    "    total_doc_len_normalized = len(clean_text_normalized(ground_truths[\"__ALL__\"]))\n",
    "    total_doc_len_nonorm = len(clean_text_nonorm(ground_truths[\"__ALL__\"]))\n",
    "\n",
    "    # -> Gather each transcribed document and put into dict:\n",
    "\n",
    "    # Structure: results[model][doc]\n",
    "    results = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        logger.info(\"Collecting results for model: %s\", model)\n",
    "        model_path = os.path.join(txt_output_dir, model_type, model)\n",
    "        results[model_type] = results.get(model_type, {})\n",
    "        results[model_type][model], results[model_type][model][\"__ALL__\"] = get_docs(model_path, doc_names, \"txt\", name_has_prefix=False)\n",
    "        logger.info(\"Collected results for model_type: %s, model: %s\", model_type, model)\n",
    "\n",
    "    # ===============\n",
    "    # Compute metrics\n",
    "    # ===============\n",
    "\n",
    "    normalized_results_data = {}\n",
    "    nonorm_results_data = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        normalized_results_data[model_type] = normalized_results_data.get(model_type, {})\n",
    "        normalized_results_data[model_type][model] = normalized_results_data[model_type].get(model, {})\n",
    "        nonorm_results_data[model_type] = nonorm_results_data.get(model_type, {})\n",
    "        nonorm_results_data[model_type][model] = nonorm_results_data[model_type].get(model, {})\n",
    "\n",
    "        logger.info(\"Computing metrics for model_type: %s, model: %s\", model_type, model)\n",
    "        for doc in doc_names:\n",
    "            logger.info(\"Computing metrics for document: %s\", doc)\n",
    "            normalized_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], \"txt\", normalized=True\n",
    "            )\n",
    "            nonorm_results_data[model_type][model][doc] = compute_metrics(\n",
    "                ground_truths[doc], results[model_type][model][doc], \"txt\", normalized=False\n",
    "            )\n",
    "\n",
    "        normalized_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], \"txt\", normalized=True\n",
    "        )\n",
    "        nonorm_results_data[model_type][model][\"__ALL__\"] = compute_metrics(\n",
    "            ground_truths[\"__ALL__\"], results[model_type][model][\"__ALL__\"], \"txt\", normalized=False\n",
    "        )\n",
    "\n",
    "    # Compute metrics separately for [__ALL__]\n",
    "\n",
    "    # ====================\n",
    "    # Put metrics in table\n",
    "    # ====================\n",
    "\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    results_base_dir = root_dir / \"benchmarking-results\" / f\"txt-accuracy\"\n",
    "\n",
    "    # Create different results directory for each model type\n",
    "    for model_type, _ in all_models:\n",
    "        results_dir = results_base_dir / model_type\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        normalized_df = build_dataframe(\n",
    "            f\"normalized_{time}\",\n",
    "            doc_names,\n",
    "            normalized_results_data[model_type],\n",
    "            doc_lengths_normalized,\n",
    "            total_doc_len_normalized,\n",
    "        )\n",
    "        nonorm_df = build_dataframe(\n",
    "            f\"nonorm_{time}\",\n",
    "            doc_names,\n",
    "            nonorm_results_data[model_type],\n",
    "            doc_lengths_nonorm,\n",
    "            total_doc_len_nonorm,\n",
    "        )\n",
    "\n",
    "        # ============\n",
    "        # Save results\n",
    "        # ============\n",
    "\n",
    "        # # Default save to project_root/benchmarking-results/txt-accuracy\n",
    "        # results_path = os.path.join(project_root, \"benchmarking-results\", \"txt-accuracy\")\n",
    "        # if not os.path.exists(results_path):\n",
    "        #     os.makedirs(results_path)\n",
    "        normalized_df.to_csv(os.path.join(str(results_dir), f\"normalized_{time}.csv\"))\n",
    "        nonorm_df.to_csv(os.path.join(str(results_dir), f\"nonorm_{time}.csv\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c620e",
   "metadata": {},
   "source": [
    "b. JSON benchmarking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d409d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().\n",
    "parent))\n",
    "from benchmarking.json_accuracy import filter_expected_columns, build_dataframe, compare_dataframes_normalized, compare_dataframes_exact, compare_dataframes_fuzzy\n",
    "from tools.file_retrieval import get_doc_names, get_docs, get_all_models\n",
    "from venv import logger\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Prerequisites:\n",
    "    - Ground truth JSON files located at `project_root/ground-truth/json/gt_kbaa-pXYZ.json`\n",
    "    - LLM/OCR transcribed JSON files located at:\n",
    "        - for ground truth text to JSON via LLM:\n",
    "            - `project_root/results/gt-txt2json/<MODEL-NAME>/<MODEL-NAME>_img_kbaa-pXYZ.json`\n",
    "        - for OCR text to JSON via LLM:\n",
    "            - `project_root/results/ocr-txt2json/<MODEL-NAME>/<MODEL-NAME>_img_kbaa-pXYZ.json`\n",
    "        - for image to JSON via LLM:\n",
    "            - `project_root/results/llm-img2json/<MODEL-NAME>/<MODEL-NAME>_img_kbaa-pXYZ.json`\n",
    "        - for text to JSON via LLM:\n",
    "            - `project_root/results/llm-txt2json/<MODEL-NAME>/<MODEL-NAME>_img_kbaa-pXYZ.json`\n",
    "\n",
    "    The main function will:\n",
    "    - Gather all ground truth JSON files\n",
    "    - For each ground truth JSON file and for each LLM/OCR model, open the JSON file's entries object as a Pandas dataframe\n",
    "    - Clean all the JSON files (either basic cleaning and normalization)\n",
    "    - Compute metrics for each file and model\n",
    "    - Save results in two CSV files (one for normalized, one for non-normalized)\n",
    "        - Results are saved in `project_root/benchmarking-results/txt-accuracy`\n",
    "    \"\"\"\n",
    "\n",
    "    # =============\n",
    "    # Preliminaries\n",
    "    # =============\n",
    "\n",
    "    #logger.info(\"Script directory: %s\", script_dir)\n",
    "    logger.info(\"Project root: %s\", root_dir)\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth_dir = os.path.join(root_dir, \"data\", \"ground-truth\", \"json\")\n",
    "    doc_names = get_doc_names(ground_truth_dir, \"json\", keep_prefix=False)\n",
    "\n",
    "    # results/ paths\n",
    "    all_models = get_all_models( \"json\",\n",
    "        os.path.join(root_dir, \"results\", \"json\", \"llm-img2json\"),\n",
    "        os.path.join(root_dir, \"results\", \"json\", \"llm-txt2json\")\n",
    "    )\n",
    "    logger.info(f\"Models found: {all_models}\")\n",
    "\n",
    "    # ===========\n",
    "    # Gather files\n",
    "    # ===========\n",
    "\n",
    "    # -> Gather ground truths and put into dict:\n",
    "\n",
    "    ground_truths_json, _ = get_docs(\n",
    "        ground_truth_dir, doc_names, \"json\", name_has_prefix=True\n",
    "    )\n",
    "\n",
    "    logger.info(\"Collected ground truth results: %s\", list(ground_truths_json.keys()))\n",
    "\n",
    "    # Convert JSON to dataframe\n",
    "\n",
    "    ground_truths_df = {\n",
    "        doc_name: filter_expected_columns(pd.DataFrame(doc_json['entries'])) for doc_name, doc_json in ground_truths_json.items()\n",
    "    }\n",
    "\n",
    "    logger.info(\"Converted ground truths to dataframes\")\n",
    "\n",
    "    # -> Gather each transcribed document and put into dict:\n",
    "\n",
    "    # Structure: results[(model_type, model)][doc]\n",
    "    results_json = {} # Stores collected outputs as JSON\n",
    "    results_df = {} # Stores collected outputs as dataframes\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        logger.info(\"Collecting results for model: %s/%s\", model_type, model)\n",
    "\n",
    "        model_path = os.path.join(root_dir, \"results\", \"json\", model_type, model)\n",
    "        print(model_path)\n",
    "        results_json[(model_type, model)], _ = get_docs(\n",
    "            model_path, doc_names, \"json\", name_has_prefix=True\n",
    "        )\n",
    "\n",
    "        logger.info(\"Collected results for model: %s\", list(results_json[(model_type, model)].keys()))\n",
    "\n",
    "        results_df[(model_type, model)] = {\n",
    "            doc_name: filter_expected_columns(pd.DataFrame(doc_json['entries'])) for doc_name, doc_json in results_json[(model_type, model)].items()\n",
    "        }\n",
    "\n",
    "        logger.info(\"Converted results to dataframes\")\n",
    "\n",
    "\n",
    "    # ===============\n",
    "    # Compute metrics\n",
    "    # ===============\n",
    "\n",
    "    normalized_results_data = {}\n",
    "    nonorm_results_data = {}\n",
    "    fuzzy_results_data = {}\n",
    "\n",
    "    for model_type, model in all_models:\n",
    "        normalized_results_data[model_type] = normalized_results_data.get(model_type, {})\n",
    "        normalized_results_data[model_type][model] = normalized_results_data[model_type].get(model, {})\n",
    "\n",
    "        nonorm_results_data[model_type] = nonorm_results_data.get(model_type, {})\n",
    "        nonorm_results_data[model_type][model] = nonorm_results_data[model_type].get(model, {})\n",
    "\n",
    "        fuzzy_results_data[model_type] = fuzzy_results_data.get(model_type, {})\n",
    "        fuzzy_results_data[model_type][model] = fuzzy_results_data[model_type].get(model, {})\n",
    "        \n",
    "        logger.info(\"Computing metrics for model: %s\", model)\n",
    "\n",
    "        for doc in doc_names:\n",
    "            logger.info(\"Computing metrics for document: %s\", doc)\n",
    "\n",
    "            normalized_results_data[model_type][model][doc] = compare_dataframes_normalized(\n",
    "                ground_truths_df[doc], results_df[(model_type, model)][doc]\n",
    "            )\n",
    "            nonorm_results_data[model_type][model][doc] = compare_dataframes_exact(\n",
    "                ground_truths_df[doc], results_df[(model_type, model)][doc]\n",
    "            )\n",
    "            fuzzy_results_data[model_type][model][doc] = compare_dataframes_fuzzy(\n",
    "                ground_truths_df[doc], results_df[(model_type, model)][doc]\n",
    "            )\n",
    "\n",
    "\n",
    "    # =====================================\n",
    "    # Put metrics in table and save results\n",
    "    # =====================================\n",
    "\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    # Iterate over model types:\n",
    "    for model_type in normalized_results_data.keys():\n",
    "        normalized_df = build_dataframe(f\"{model_type}_normalized_{time}\", doc_names, normalized_results_data[model_type])\n",
    "        nonorm_df = build_dataframe(f\"{model_type}_nonorm_{time}\", doc_names, nonorm_results_data[model_type])\n",
    "        fuzzy_df = build_dataframe(f\"{model_type}_fuzzy_{time}\", doc_names, fuzzy_results_data[model_type])\n",
    "\n",
    "        results_path = os.path.join(root_dir, \"benchmarking-results\", \"json-accuracy\", model_type)\n",
    "        if not os.path.exists(results_path):\n",
    "            os.makedirs(results_path)\n",
    "\n",
    "        normalized_df.to_csv(os.path.join(results_path, f\"{model_type}_normalized_{time}.csv\"))\n",
    "        nonorm_df.to_csv(os.path.join(results_path, f\"{model_type}_nonorm_{time}.csv\"))\n",
    "        fuzzy_df.to_csv(os.path.join(results_path, f\"{model_type}_fuzzy_{time}.csv\"))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
